import openai
import os
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func, text
from dotenv import load_dotenv
from aibackend.app.news_vector import chunkVector
from aibackend.app.news_vector_db import get_news_db

load_dotenv()

class RAGService:
    def __init__(self):
        self.openai_client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            timeout=60.0
        )
        self.embedding_model = "text-embedding-3-small"
        self.chat_model = "gpt-4o"
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        if os.getenv("OPENAI_API_KEY") is None:
            print("âŒ OPENAI_API_KEY is None", flush=True)
        try:
            print("ì„ë² ë”© ì‹œì‘", flush=True)
            response = self.openai_client.embeddings.create(
                input=[text],
                model=self.embedding_model,
                timeout=30
            )
            print("ì„ë² ë”© ì™„ë£Œ", flush=True)
            return response.data[0].embedding
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}", flush=True)
            return []
    
    def similarity_search(
        self, 
        query: str, 
        db: Session, 
        top_k: int = 100,
        similarity_threshold: float = 0.5
    ) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ê¸°"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            print(f"ë²¡í„° ê²€ìƒ‰ ì‹œì‘ - ì¿¼ë¦¬: {query}", flush=True)
            query_embedding = self.get_embedding(query)
            if not query_embedding:
                print("ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ", flush=True)
                return []
            
            print(f"ë²¡í„° ê²€ìƒ‰ ì‹œì‘ - ì„ë² ë”© ê¸¸ì´: {len(query_embedding)}", flush=True)
            
            # ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            count_query = text("SELECT COUNT(*) as total FROM news_chunks")
            count_result = db.execute(count_query).fetchone()
            total_chunks = count_result.total if count_result else 0
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ ì²­í¬ ìˆ˜: {total_chunks}", flush=True)
            
            if total_chunks == 0:
                print("âŒ news_chunks í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!", flush=True)
                return []
            
            # pgvectorì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
            # ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
            embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
            print(f"ë²¡í„° ë¬¸ìì—´ ìƒì„± ì™„ë£Œ - ê¸¸ì´: {len(embedding_str)}", flush=True)
            
            # ë¨¼ì € ì„ê³„ê°’ ì—†ì´ ìƒìœ„ ê²°ê³¼ë“¤ í™•ì¸
            debug_query = text("""
                SELECT id, title, chunk_text, published_at,
                       1 - (embedding <=> CAST(:query_embedding AS vector)) as similarity
                FROM news_chunks
                ORDER BY embedding <=> CAST(:query_embedding AS vector)
                LIMIT 5
            """)
            
            print("ë””ë²„ê·¸ìš© ìƒìœ„ 5ê°œ ê²°ê³¼ í™•ì¸ ì‹œì‘", flush=True)
            debug_result = db.execute(debug_query, {"query_embedding": embedding_str})
            
            debug_docs = []
            for row in debug_result:
                debug_docs.append({
                    "similarity": float(row.similarity),
                    "title": row.title[:100] + "..." if len(row.title) > 100 else row.title
                })
            
            print(f"ìƒìœ„ 5ê°œ ê²°ê³¼ì˜ ìœ ì‚¬ë„: {[doc['similarity'] for doc in debug_docs]}", flush=True)
            print(f"ìƒìœ„ 5ê°œ ê²°ê³¼ì˜ ì œëª©: {[doc['title'] for doc in debug_docs]}", flush=True)
            
            # ë™ì  ì„ê³„ê°’ ì¡°ì •
            if debug_docs:
                max_similarity = max([doc['similarity'] for doc in debug_docs])
                print(f"ìµœê³  ìœ ì‚¬ë„: {max_similarity}", flush=True)
                
                # ìµœê³  ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì„ê³„ê°’ì„ ë‚®ì¶¤
                if max_similarity < similarity_threshold:
                    adjusted_threshold = max(0.1, max_similarity - 0.1)
                    print(f"ì„ê³„ê°’ ì¡°ì •: {similarity_threshold} -> {adjusted_threshold}", flush=True)
                    similarity_threshold = adjusted_threshold
            
            # ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬
            query_text = text("""
                SELECT id, title, chunk_text, published_at,
                       1 - (embedding <=> CAST(:query_embedding AS vector)) as similarity
                FROM news_chunks
                WHERE 1 - (embedding <=> CAST(:query_embedding AS vector)) > :threshold
                ORDER BY embedding <=> CAST(:query_embedding AS vector)
                LIMIT :limit
            """)
            
            print(f"SQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘ - ì„ê³„ê°’: {similarity_threshold}", flush=True)
            result = db.execute(
                query_text,
                {
                    "query_embedding": embedding_str,
                    "threshold": similarity_threshold,
                    "limit": top_k
                }
            )
            print("SQL ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ", flush=True)
            
            similar_docs = []
            row_count = 0
            for row in result:
                row_count += 1
                similar_docs.append({
                    "id": str(row.id),
                    "title": row.title,
                    "content": row.chunk_text,
                    "published_at": row.published_at.isoformat() if row.published_at else None,
                    "similarity": float(row.similarity)
                })
            
            print(f"ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ - {row_count}ê°œ í–‰, ì„ê³„ê°’: {similarity_threshold}", flush=True)
            if similar_docs:
                print(f"ë°˜í™˜ëœ ê²°ê³¼ì˜ ìœ ì‚¬ë„ ë²”ìœ„: {min([doc['similarity'] for doc in similar_docs]):.3f} ~ {max([doc['similarity'] for doc in similar_docs]):.3f}", flush=True)
            
            return similar_docs
            
        except Exception as e:
            print(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True)
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", flush=True)
            return []
    
    def _generate_diverse_queries(self, stock_code: str, stock=None) -> List[Dict[str, Any]]:
        """ì£¼ì‹ ë¶„ì„ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = []
        
        # 1. ê¸°ë³¸ ì¢…ëª© ì½”ë“œ ê²€ìƒ‰ (ë†’ì€ ì •í™•ë„)
        queries.append({
            "query": stock_code,
            "type": "stock_code",
            "top_k": 25,  # 10 â†’ 25
            "threshold": 0.6  # 0.7 â†’ 0.6
        })
        
        if stock:
            # 2. íšŒì‚¬ëª… ê²€ìƒ‰ (ì¤‘ê°„ ì •í™•ë„)
            if stock.company_name:
                queries.append({
                    "query": stock.company_name,
                    "type": "company_name", 
                    "top_k": 20,  # 8 â†’ 20
                    "threshold": 0.5  # 0.6 â†’ 0.5
                })
                
                # 3. íšŒì‚¬ëª… + ì£¼ìš” í‚¤ì›Œë“œ ì¡°í•©
                company_keywords = [
                    f"{stock.company_name} ì‹¤ì ",
                    f"{stock.company_name} ì „ë§",
                    f"{stock.company_name} íˆ¬ì",
                    f"{stock.company_name} ì£¼ê°€"
                ]
                for keyword in company_keywords:
                    queries.append({
                        "query": keyword,
                        "type": "company_keyword",
                        "top_k": 15,  # 5 â†’ 15
                        "threshold": 0.4  # 0.5 â†’ 0.4
                    })
            
            # 4. ì—…ì¢…/ì„¹í„° ê´€ë ¨ ê²€ìƒ‰ (ë‚®ì€ ì •í™•ë„, ë„“ì€ ë²”ìœ„)
            if stock.industry:
                queries.append({
                    "query": f"{stock.industry} ì—…ì¢…",
                    "type": "industry",
                    "top_k": 12,  # 5 â†’ 12
                    "threshold": 0.3  # 0.4 â†’ 0.3
                })
                
            if stock.sector:
                queries.append({
                    "query": f"{stock.sector} ì„¹í„°",
                    "type": "sector", 
                    "top_k": 12,  # 5 â†’ 12
                    "threshold": 0.3  # 0.4 â†’ 0.3
                })
            
            # 5. ì‚¬ì—… ì˜ì—­ ê´€ë ¨ ê²€ìƒ‰
            if stock.business_summary:
                # ì‚¬ì—… ìš”ì•½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                business_keywords = self._extract_business_keywords(stock.business_summary)
                for keyword in business_keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ
                    queries.append({
                        "query": keyword,
                        "type": "business_keyword",
                        "top_k": 10,  # 4 â†’ 10
                        "threshold": 0.3  # 0.4 â†’ 0.3
                    })
        
        # 6. ì¼ë°˜ì ì¸ ì£¼ì‹ ê´€ë ¨ í‚¤ì›Œë“œ (ì¢…ëª©ì½”ë“œì™€ í•¨ê»˜)
        general_keywords = [
            f"{stock_code} ë¶„ì„",
            f"{stock_code} ë¦¬í¬íŠ¸", 
            f"{stock_code} ëª©í‘œê°€",
            f"{stock_code} ì¶”ì²œ"
        ]
        for keyword in general_keywords:
            queries.append({
                "query": keyword,
                "type": "analysis_keyword",
                "top_k": 8,  # 3 â†’ 8
                "threshold": 0.4  # 0.5 â†’ 0.4
            })
        
        return queries
    
    def _extract_business_keywords(self, business_summary: str) -> List[str]:
        """ì‚¬ì—… ìš”ì•½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not business_summary:
            return []
        
        # ì£¼ìš” ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œë“¤
        business_terms = [
            "ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í”Œë ˆì´", "ìŠ¤ë§ˆíŠ¸í°", "ì „ì", "IT", "ì†Œí”„íŠ¸ì›¨ì–´",
            "ë°”ì´ì˜¤", "ì œì•½", "í™”í•™", "ì„ìœ ", "ìë™ì°¨", "ì¡°ì„ ", "ê±´ì„¤", "ê¸ˆìœµ",
            "ì€í–‰", "ì¦ê¶Œ", "ë³´í—˜", "í†µì‹ ", "ê²Œì„", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ìœ í†µ",
            "ì‹í’ˆ", "ìŒë£Œ", "ì˜ë¥˜", "í™”ì¥í’ˆ", "í•­ê³µ", "ë¬¼ë¥˜", "ì—ë„ˆì§€",
            "AI", "ì¸ê³µì§€ëŠ¥", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "5G", "IoT", "ë¸”ë¡ì²´ì¸"
        ]
        
        found_keywords = []
        business_lower = business_summary.lower()
        
        for term in business_terms:
            if term in business_summary or term.lower() in business_lower:
                found_keywords.append(term)
        
        return found_keywords[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€

    
    def generate_stock_analysis(
        self,
        stock_code: str,
        news_db: Session = None,
        main_db: Session = None
    ) -> Dict[str, Any]:
        """ì£¼ì‹ ë¶„ì„ìš© íŠ¹í™” RAG (ë‰´ìŠ¤ + ì¬ë¬´ ë°ì´í„°)"""
        try:
            # 1. ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
            similar_docs = []
            if news_db and main_db:
                # ê¸°ì—… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                from ..models import Stock
                stock = main_db.query(Stock).filter(Stock.code == stock_code).first()
                
                # ë‹¤ì±„ë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                print(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì‹œì‘: {stock_code}", flush=True)
                search_queries = self._generate_diverse_queries(stock_code, stock)
                print(f"ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(search_queries)}", flush=True)
                
                # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ ìˆ˜ì§‘
                all_docs = []
                for i, query_info in enumerate(search_queries):
                    print(f"ê²€ìƒ‰ {i+1}/{len(search_queries)}: {query_info['query']}", flush=True)
                    docs = self.similarity_search(
                        query=query_info["query"],
                        db=news_db,
                        top_k=query_info["top_k"],
                        similarity_threshold=query_info["threshold"]
                    )
                    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ", flush=True)
                    # ì¿¼ë¦¬ íƒ€ì… ì •ë³´ ì¶”ê°€
                    for doc in docs:
                        doc["query_type"] = query_info["type"]
                    all_docs.extend(docs)
                
                # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
                seen_ids = set()
                similar_docs = []
                for doc in all_docs:
                    if doc["id"] not in seen_ids:
                        seen_ids.add(doc["id"])
                        similar_docs.append(doc)
                
                # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 40ê°œë§Œ ì„ íƒ (ì²­í¬ ê¸°ë°˜ì´ë¯€ë¡œ ë” ë§ì´)
                similar_docs = sorted(similar_docs, key=lambda x: x["similarity"], reverse=True)[:40]
                print(similar_docs)
            
            elif news_db:
                # DB ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ” ê¸°ë³¸ ê²€ìƒ‰
                similar_docs = self.similarity_search(
                    query=stock_code,
                    db=news_db,
                    top_k=30,  # 15 â†’ 30
                    similarity_threshold=0.4  # 0.5 â†’ 0.4
                )
            
            # ë‰´ìŠ¤ê°€ ì—†ì–´ë„ ì¬ë¬´ ë°ì´í„°ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰
            news_content = ""
            if similar_docs:
                # ë‰´ìŠ¤ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
                news_summaries = []
                for doc in similar_docs:
                    news_summaries.append(f"â€¢ {doc['title']}: {doc['content'][:200]}...")
                print(news_summaries)
                news_content = "\n".join(news_summaries)
            
            # 2. ê¸°ì—… ì¢…í•© ë¶„ì„ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            analysis_prompt = """ë‹¹ì‹ ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ ì¶œì‹ ì˜ ì „ë¬¸ ê¸°ì—… ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë‰´ìŠ¤ ë°ì´í„°ì™€ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“Œ ì£¼ì–´ì§€ëŠ” ë°ì´í„°:
- ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤
- ìƒì„¸í•œ ì¬ë¬´ ì •ë³´ (ì¬ë¬´ì œí‘œ, ì‹œì¥ì§€í‘œ, ì‹¤ì ì „ë§)

ğŸ¯ ë¶„ì„ ë³´ê³ ì„œ êµ¬ì„±:

## ğŸ“Š ê¸°ì—… ê°œìš”
- ì‚¬ì—… ë¶„ì•¼ ë° ì£¼ìš” ì‚¬ì—…
- ì—…ê³„ ë‚´ ìœ„ì¹˜

## ğŸ“ˆ ì¬ë¬´ ìƒí™© ë¶„ì„
- ìˆ˜ìµì„± ì§€í‘œ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ë§ˆì§„ë¥  ë“±)
- ì¬ë¬´ê±´ì „ì„± (ë¶€ì±„ë¹„ìœ¨, ìœ ë™ë¹„ìœ¨, ROE/ROA ë“±)
- ì„±ì¥ì„± (ë§¤ì¶œ/ì´ìµ ì¦ê°€ìœ¨)
- ë°¸ë¥˜ì—ì´ì…˜ (PER, PBR, EV/EBITDA ë“±)

## ğŸ“° ìµœê·¼ ë™í–¥ ë° ì´ìŠˆ
- ì£¼ìš” ë‰´ìŠ¤ ì´ë²¤íŠ¸ ìš”ì•½
- ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ ë¶„ì„
- ì‹œì¥ ë°˜ì‘ ë° ì˜í–¥ë„

## ğŸ”® ì „ë§ ë° ë¦¬ìŠ¤í¬
- ì‹¤ì  ì „ë§
- ì„±ì¥ ë™ë ¥ ë° ê¸°íšŒìš”ì¸
- ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸

## ğŸ’¡ ì¢…í•© ì˜ê²¬
- íˆ¬ì ë§¤ë ¥ë„: â­â­â­â­â­ (5ì  ë§Œì )
- íˆ¬ì í¬ì¸íŠ¸ ìš”ì•½
- ì£¼ì˜ ì‚¬í•­

âš ï¸ ì‘ì„± ì›ì¹™:
- ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê° ìœ ì§€
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±° ì œì‹œ
- íˆ¬ì ìœ„í—˜ì„± ëª…ì‹œ
- ì •ë³´ ì œê³µ ì¤‘ì‹¬ (êµ¬ì²´ì  ë§¤ë§¤ ì¡°ì–¸ ì§€ì–‘)"""
            
            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\n\n".join([
                f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['content']}\në°œí–‰ì¼: {doc['published_at']}"
                for doc in similar_docs
            ])
            
            # 4. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            user_content = f"""
[ë¶„ì„ ëŒ€ìƒ ì¢…ëª©]: {stock_code}

[ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤]:
{context_text if similar_docs else "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"}
"""
            
            # ì¬ë¬´ ë°ì´í„° ìë™ ê°€ì ¸ì˜¤ê¸° ë° ì¶”ê°€
            if main_db:
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¬ë¬´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                from ..models import Stock, FinancialStatement, MarketIndicator, EarningsForecast
                
                # ì¢…ëª© ê¸°ë³¸ ì •ë³´
                stock = main_db.query(Stock).filter(Stock.code == stock_code).first()
                if stock:
                    # ì¬ë¬´ì œí‘œ (ìµœì‹ ìˆœ)
                    financials = main_db.query(FinancialStatement).filter(
                        FinancialStatement.stock_code == stock_code
                    ).order_by(FinancialStatement.report_period.desc()).limit(4).all()
                    
                    # ì‹œì¥ì§€í‘œ (ìµœì‹ )
                    latest_indicator = main_db.query(MarketIndicator).filter(
                        MarketIndicator.stock_code == stock_code
                    ).order_by(MarketIndicator.date.desc()).first()
                    
                    # ì‹¤ì ì „ë§
                    forecasts = main_db.query(EarningsForecast).filter(
                        EarningsForecast.stock_code == stock_code
                    ).order_by(EarningsForecast.fiscal_year.desc()).limit(3).all()
                    
                    # ì¬ë¬´ ë°ì´í„° ë¬¸ìì—´ êµ¬ì„±
                    financial_data_parts = []
                    
                    # ê¸°ì—… ê¸°ë³¸ ì •ë³´ (ëª¨ë“  í•„ë“œ)
                    financial_data_parts.append(f"â—† ê¸°ì—…ëª…: {stock.company_name}")
                    if stock.industry:
                        financial_data_parts.append(f"â—† ì—…ì¢…: {stock.industry}")
                    if stock.sector:
                        financial_data_parts.append(f"â—† ì„¹í„°: {stock.sector}")
                    if stock.business_summary:
                        financial_data_parts.append(f"â—† ì‚¬ì—…ê°œìš”: {stock.business_summary}")
                    if stock.financial_info:
                        financial_data_parts.append(f"â—† ì¬ë¬´ì •ë³´: {stock.financial_info}")
                    
                    # ì¬ë¬´ì œí‘œ ë°ì´í„° (ëª¨ë“  í•„ë“œ)
                    if financials:
                        financial_data_parts.append("\nâ—† ì¬ë¬´ì œí‘œ:")
                        for f in financials:
                            financial_data_parts.append(f"  - {f.report_period} ({f.report_type})")
                            # ì†ìµê³„ì‚°ì„œ í•­ëª©
                            if f.revenue:
                                financial_data_parts.append(f"    ë§¤ì¶œì•¡: {f.revenue:,}ì›")
                            if f.operating_income:
                                financial_data_parts.append(f"    ì˜ì—…ì´ìµ: {f.operating_income:,}ì›")
                            if f.net_income:
                                financial_data_parts.append(f"    ìˆœì´ìµ: {f.net_income:,}ì›")
                            if f.gross_profits:
                                financial_data_parts.append(f"    ë§¤ì¶œì´ì´ìµ: {f.gross_profits:,}ì›")
                            if f.ebitda:
                                financial_data_parts.append(f"    EBITDA: {f.ebitda:,}ì›")
                            
                            # ì¬ë¬´ìƒíƒœí‘œ í•­ëª©
                            if f.assets:
                                financial_data_parts.append(f"    ì´ìì‚°: {f.assets:,}ì›")
                            if f.liabilities:
                                financial_data_parts.append(f"    ì´ë¶€ì±„: {f.liabilities:,}ì›")
                            if f.equity:
                                financial_data_parts.append(f"    ìë³¸ì´ê³„: {f.equity:,}ì›")
                            
                            # í˜„ê¸ˆíë¦„í‘œ í•­ëª©
                            if f.operating_cashflow:
                                financial_data_parts.append(f"    ì˜ì—…í˜„ê¸ˆíë¦„: {f.operating_cashflow:,}ì›")
                            if f.free_cashflow:
                                financial_data_parts.append(f"    ì‰ì—¬í˜„ê¸ˆíë¦„: {f.free_cashflow:,}ì›")
                            
                            # ì£¼ë‹¹ ì§€í‘œ
                            if f.revenue_per_share:
                                financial_data_parts.append(f"    ì£¼ë‹¹ë§¤ì¶œ: {float(f.revenue_per_share):,.3f}ì›")
                            
                            # ìˆ˜ìµì„± ì§€í‘œ
                            if f.gross_margins:
                                financial_data_parts.append(f"    ë§¤ì¶œì´ì´ìµë¥ : {float(f.gross_margins)*100:.2f}%")
                            if f.ebitda_margins:
                                financial_data_parts.append(f"    EBITDA ë§ˆì§„: {float(f.ebitda_margins)*100:.2f}%")
                            if f.operating_margins:
                                financial_data_parts.append(f"    ì˜ì—…ì´ìµë¥ : {float(f.operating_margins)*100:.2f}%")
                            if f.return_on_assets:
                                financial_data_parts.append(f"    ROA: {float(f.return_on_assets)*100:.2f}%")
                            if f.return_on_equity:
                                financial_data_parts.append(f"    ROE: {float(f.return_on_equity)*100:.2f}%")
                            
                            # ì•ˆì •ì„± ì§€í‘œ
                            if f.debt_to_equity:
                                financial_data_parts.append(f"    ë¶€ì±„ë¹„ìœ¨: {float(f.debt_to_equity):.2f}")
                            if f.quick_ratio:
                                financial_data_parts.append(f"    ë‹¹ì¢Œë¹„ìœ¨: {float(f.quick_ratio):.2f}")
                            if f.current_ratio:
                                financial_data_parts.append(f"    ìœ ë™ë¹„ìœ¨: {float(f.current_ratio):.2f}")
                            
                            # ì„±ì¥ì„± ì§€í‘œ
                            if f.earnings_growth:
                                financial_data_parts.append(f"    ìˆœì´ìµ ì¦ê°€ìœ¨: {float(f.earnings_growth)*100:.2f}%")
                            if f.revenue_growth:
                                financial_data_parts.append(f"    ë§¤ì¶œ ì¦ê°€ìœ¨: {float(f.revenue_growth)*100:.2f}%")
                            
                            # ê¸°ì—…ê°€ì¹˜ ì§€í‘œ
                            if f.enterprise_value:
                                financial_data_parts.append(f"    ê¸°ì—…ê°€ì¹˜(EV): {f.enterprise_value:,}ì›")
                            if f.enterprise_to_revenue:
                                financial_data_parts.append(f"    EV/ë§¤ì¶œ: {float(f.enterprise_to_revenue):.2f}")
                            if f.enterprise_to_ebitda:
                                financial_data_parts.append(f"    EV/EBITDA: {float(f.enterprise_to_ebitda):.2f}")
                            
                            financial_data_parts.append("")
                    
                    # ì‹œì¥ì§€í‘œ ë°ì´í„° (ëª¨ë“  í•„ë“œ)
                    if latest_indicator:
                        financial_data_parts.append("â—† ì‹œì¥ì§€í‘œ (ìµœê·¼ ë°ì´í„°):")
                        financial_data_parts.append(f"  - ê¸°ì¤€ì¼: {latest_indicator.date}")
                        
                        # ì£¼ê°€ ì •ë³´
                        if latest_indicator.current_price:
                            financial_data_parts.append(f"    í˜„ì¬ê°€: {float(latest_indicator.current_price):,}ì›")
                        if latest_indicator.previous_close:
                            financial_data_parts.append(f"    ì „ì¼ì¢…ê°€: {float(latest_indicator.previous_close):,}ì›")
                        if latest_indicator.open:
                            financial_data_parts.append(f"    ì‹œê°€: {float(latest_indicator.open):,}ì›")
                        if latest_indicator.close_price:
                            financial_data_parts.append(f"    ì¢…ê°€: {latest_indicator.close_price:,}ì›")
                        if latest_indicator.day_low:
                            financial_data_parts.append(f"    ì¼ ìµœì €ê°€: {float(latest_indicator.day_low):,}ì›")
                        if latest_indicator.day_high:
                            financial_data_parts.append(f"    ì¼ ìµœê³ ê°€: {float(latest_indicator.day_high):,}ì›")
                        if latest_indicator.fifty_two_week_low:
                            financial_data_parts.append(f"    52ì£¼ ìµœì €ê°€: {float(latest_indicator.fifty_two_week_low):,}ì›")
                        if latest_indicator.fifty_two_week_high:
                            financial_data_parts.append(f"    52ì£¼ ìµœê³ ê°€: {float(latest_indicator.fifty_two_week_high):,}ì›")
                        if latest_indicator.fifty_two_week_change_percent:
                            financial_data_parts.append(f"    52ì£¼ ë³€ë™ë¥ : {float(latest_indicator.fifty_two_week_change_percent)*100:.2f}%")
                        
                        # ê±°ë˜ëŸ‰ ì •ë³´
                        if latest_indicator.volume:
                            financial_data_parts.append(f"    ê±°ë˜ëŸ‰: {latest_indicator.volume:,}ì£¼")
                        if latest_indicator.average_volume:
                            financial_data_parts.append(f"    í‰ê· ê±°ë˜ëŸ‰: {latest_indicator.average_volume:,}ì£¼")
                        
                        # ì‹œì¥ ì •ë³´
                        if latest_indicator.market_cap:
                            financial_data_parts.append(f"    ì‹œê°€ì´ì•¡: {latest_indicator.market_cap:,}ì›")
                        if latest_indicator.market:
                            financial_data_parts.append(f"    ì‹œì¥: {latest_indicator.market}")
                        if latest_indicator.exchange:
                            financial_data_parts.append(f"    ê±°ë˜ì†Œ: {latest_indicator.exchange}")
                        if latest_indicator.currency:
                            financial_data_parts.append(f"    í†µí™”: {latest_indicator.currency}")
                        
                        # ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ
                        if latest_indicator.per:
                            financial_data_parts.append(f"    PER: {latest_indicator.per:.2f}")
                        if latest_indicator.pbr:
                            financial_data_parts.append(f"    PBR: {latest_indicator.pbr:.2f}")
                        if latest_indicator.pe_ratio_trailing:
                            financial_data_parts.append(f"    PER(í›„í–‰): {float(latest_indicator.pe_ratio_trailing):.2f}")
                        if latest_indicator.pe_ratio_forward:
                            financial_data_parts.append(f"    PER(ì„ í–‰): {float(latest_indicator.pe_ratio_forward):.2f}")
                        
                        # ì£¼ë‹¹ ì§€í‘œ
                        if latest_indicator.eps:
                            financial_data_parts.append(f"    EPS: {latest_indicator.eps:,}ì›")
                        if latest_indicator.eps_forward:
                            financial_data_parts.append(f"    EPS(ì„ í–‰): {float(latest_indicator.eps_forward):,}ì›")
                        if latest_indicator.eps_current_year:
                            financial_data_parts.append(f"    EPS(ë‹¹í•´ë…„ë„): {float(latest_indicator.eps_current_year):,}ì›")
                        if latest_indicator.price_eps_current_year:
                            financial_data_parts.append(f"    ì£¼ê°€/EPS(ë‹¹í•´ë…„ë„): {float(latest_indicator.price_eps_current_year):.2f}")
                        if latest_indicator.bps:
                            financial_data_parts.append(f"    BPS: {latest_indicator.bps:,}ì›")
                        if latest_indicator.book_value:
                            financial_data_parts.append(f"    ì¥ë¶€ê°€ì¹˜: {float(latest_indicator.book_value):,}ì›")
                        
                        # ë°°ë‹¹ ì •ë³´
                        if latest_indicator.dividend_yield:
                            financial_data_parts.append(f"    ë°°ë‹¹ìˆ˜ìµë¥ : {latest_indicator.dividend_yield:.2f}%")
                        if latest_indicator.dividend_rate:
                            financial_data_parts.append(f"    ë°°ë‹¹ë¥ : {float(latest_indicator.dividend_rate):.2f}%")
                        if latest_indicator.dividend_date:
                            financial_data_parts.append(f"    ë°°ë‹¹ì¼: {latest_indicator.dividend_date}")
                        if latest_indicator.ex_dividend_date:
                            financial_data_parts.append(f"    ë°°ë‹¹ë½ì¼: {latest_indicator.ex_dividend_date}")
                        if latest_indicator.payout_ratio:
                            financial_data_parts.append(f"    ë°°ë‹¹ì„±í–¥: {float(latest_indicator.payout_ratio)*100:.2f}%")
                        
                        # ë¦¬ìŠ¤í¬ ì§€í‘œ
                        if latest_indicator.beta:
                            financial_data_parts.append(f"    ë² íƒ€: {float(latest_indicator.beta):.2f}")
                    
                    # ì‹¤ì ì „ë§ ë°ì´í„° (ëª¨ë“  í•„ë“œ)
                    if forecasts:
                        financial_data_parts.append("\nâ—† ì‹¤ì ì „ë§:")
                        for e in forecasts:
                            financial_data_parts.append(f"  - {e.fiscal_year}ë…„ ì „ë§")
                            if e.expected_revenue:
                                financial_data_parts.append(f"    ì˜ˆìƒë§¤ì¶œ: {e.expected_revenue:,}ì›")
                            if e.expected_operating_income:
                                financial_data_parts.append(f"    ì˜ˆìƒì˜ì—…ì´ìµ: {e.expected_operating_income:,}ì›")
                            if e.expected_eps:
                                financial_data_parts.append(f"    ì˜ˆìƒEPS: {e.expected_eps:,}ì›")
                            if e.source:
                                financial_data_parts.append(f"    ì¶œì²˜: {e.source}")
                            financial_data_parts.append("")
                    
                    financial_data = "\n".join(financial_data_parts)
                    
                    # ì¬ë¬´ ë°ì´í„°ë¥¼ user_contentì— ë°”ë¡œ ì¶”ê°€
                    user_content += f"\n[ì¬ë¬´ ì •ë³´]:\n{financial_data}\n"
            
            user_content += f"\n[ë¶„ì„ ìš”ì²­]: {stock_code} ê¸°ì—…ì— ëŒ€í•œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
            
            # 5. GPT ë¶„ì„ ì‹¤í–‰
            messages = [
                {"role": "system", "content": analysis_prompt},
                {"role": "user", "content": user_content}
            ]
            
            print(f"GPT ë¶„ì„ ì‹œì‘: {stock_code}", flush=True)
            response = self.openai_client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=10000
            )
            print(f"GPT ë¶„ì„ ì™„ë£Œ: {stock_code}", flush=True)
            
            # ìƒì„±ëœ ë³´ê³ ì„œë¥¼ DBì— ì €ì¥
            report_content = response.choices[0].message.content
            if main_db:
                try:
                    from ..models import Reports
                    from datetime import datetime
                    
                    # ìƒˆë¡œìš´ ë³´ê³ ì„œ ìƒì„±
                    new_report = Reports(
                        stock_code=stock_code,
                        report=report_content,
                        created_at=datetime.now()
                    )
                    
                    main_db.add(new_report)
                    main_db.commit()
                    main_db.refresh(new_report)
                    
                    print(f"ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {stock_code}, ID: {new_report.id}")
                    
                except Exception as e:
                    print(f"ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
                    main_db.rollback()
            
            return {
                "stock_code": stock_code,
                "query": "ê¸°ì—… ì¢…í•© ë¶„ì„",
                "response": report_content,
                "sources": similar_docs,
                "success": True
            }
            
        except Exception as e:
            print(f"ì£¼ì‹ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}", flush=True)
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", flush=True)
            return {
                "stock_code": stock_code,
                "query": "ê¸°ì—… ì¢…í•© ë¶„ì„",
                "response": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "success": False
            }

    def chat_with_rag(
        self,
        user_query: str,
        user_id: str,
        stock_code: str = None,
        news_db: Session = None,
        main_db: Session = None,
        top_k: int = 25,  # 10 â†’ 25 (ì²­í¬ ê¸°ë°˜ì´ë¯€ë¡œ ë” ë§ì´)
        similarity_threshold: float = 0.3
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ í™œìš©í•œ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
        try:
            print(f"ì±„íŒ… RAG ì‹œì‘: {user_query}", flush=True)
            
            # userIDë¡œ ì‚¬ìš©ì í…Œì´ë¸” ID ì¡°íšŒ
            user_table_id = None
            if main_db and user_id:
                from ..models import User
                user = main_db.query(User).filter(User.userID == user_id).first()
                if user:
                    user_table_id = user.id
                    print(f"ì‚¬ìš©ì ID ì¡°íšŒ ì™„ë£Œ: {user_id} -> {user_table_id}", flush=True)
                else:
                    print(f"ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {user_id}", flush=True)
            
            # 1. DBì—ì„œ ì±„íŒ… ë‚´ì—­ ê°€ì ¸ì˜¤ê¸°
            chat_history = []
            if main_db and stock_code and user_table_id:
                from ..models import ChatHistory
                recent_chats = main_db.query(ChatHistory).filter(
                    ChatHistory.user_id == user_table_id,
                    ChatHistory.stock_code == stock_code
                ).order_by(ChatHistory.created_at.desc()).limit(40).all()
                
                # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
                recent_chats.reverse()
                chat_history = [
                    {"role": chat.role.value, "content": chat.chat} 
                    for chat in recent_chats
                ]
                print(f"ê°€ì ¸ì˜¨ ì±„íŒ… ë‚´ì—­ ìˆ˜: {len(chat_history)}", flush=True)
            
            # 2. DBì—ì„œ ì£¼ì‹ ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°
            stock_report = None
            if main_db and stock_code:
                from ..models import Reports
                latest_report = main_db.query(Reports).filter(
                    Reports.stock_code == stock_code
                ).order_by(Reports.created_at.desc()).first()
                
                if latest_report:
                    stock_report = latest_report.report
                    print(f"ì£¼ì‹ ë³´ê³ ì„œ ì°¾ìŒ: {stock_code}", flush=True)
                else:
                    print(f"ì£¼ì‹ ë³´ê³ ì„œ ì—†ìŒ: {stock_code}", flush=True)
            
            # 3. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ ê²€ìƒ‰
            similar_docs = []
            if news_db:
                similar_docs = self.similarity_search(
                    query=user_query,
                    db=news_db,
                    top_k=top_k,
                    similarity_threshold=similarity_threshold
                )
                print(f"ê²€ìƒ‰ëœ ë‰´ìŠ¤ ìˆ˜: {len(similar_docs)}", flush=True)
            
            # 2. ì±„íŒ…ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            chat_system_prompt = """ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
- ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬, ë¶„ì„ ë³´ê³ ì„œ, ëŒ€í™” ë§¥ë½ì„ ì¢…í•©ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ì‚¬ì‹¤ê³¼ ë°ì´í„°ë¥¼ ì¸ìš©í•˜ì„¸ìš”
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” í”¼í•˜ì„¸ìš”
- ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”

ğŸ’¡ **ë‹µë³€**
- ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‹µë³€

âš ï¸ **ì°¸ê³ ì‚¬í•­**
- íˆ¬ì ì‹œ ê³ ë ¤í•  ì ì´ë‚˜ ì£¼ì˜ì‚¬í•­"""

            # 3. ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            news_context = ""
            if similar_docs:
                news_summaries = []
                for i, doc in enumerate(similar_docs, 1):
                    news_summaries.append(f"{i}. ì œëª©: {doc['title']}\n   ë‚´ìš©: {doc['content'][:300]}...\n   ë°œí–‰ì¼: {doc['published_at']}")
                news_context = "\n".join(news_summaries)
                print(f"ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸: {news_context}", flush=True)
            else:
                news_context = "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # 4. ëŒ€í™” ë‚´ì—­ êµ¬ì„±
            chat_context = ""
            if chat_history:
                chat_messages = []
                for chat in chat_history:  # ëª¨ë“  ì±„íŒ… ë‚´ì—­
                    role = "user" if chat["role"] == "user" else "gpt"
                    chat_messages.append(f"{role}: {chat['content']}")
                chat_context = "\n".join(chat_messages)
                print(f"ì´ì „ ëŒ€í™” ë‚´ì—­: {chat_context}", flush=True)
            else:
                chat_context = "ì´ì „ ëŒ€í™” ì—†ìŒ"
            
            # 5. ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            report_context = ""
            if stock_report and stock_code:
                report_context = f"[{stock_code} ë¶„ì„ ë³´ê³ ì„œ]:\n{stock_report}"
                print(f"ì£¼ì‹ ë³´ê³ ì„œ: {report_context}", flush=True)
            else:
                report_context = "ê´€ë ¨ ë¶„ì„ ë³´ê³ ì„œ ì—†ìŒ"
            
            # 6. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            user_content = f"""
[í˜„ì¬ ì§ˆë¬¸]: {user_query}

[ì´ì „ ëŒ€í™” ë‚´ì—­]:
{chat_context}

[ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤]:
{news_context}

{report_context}

ìœ„ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            
            # 7. GPT ì±„íŒ… ì‘ë‹µ ìƒì„±
            messages = [
                {"role": "system", "content": chat_system_prompt},
                {"role": "user", "content": user_content}
            ]
            
            print("GPT ì±„íŒ… ì‘ë‹µ ìƒì„± ì‹œì‘", flush=True)
            response = self.openai_client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=10000,
                timeout=30
            )
            print("GPT ì±„íŒ… ì‘ë‹µ ìƒì„± ì™„ë£Œ", flush=True)
            
            chat_response = response.choices[0].message.content
            
            return {
                "user_query": user_query,
                "response": chat_response,
                "sources": similar_docs,
                "source_count": len(similar_docs),
                "success": True
            }
            
        except Exception as e:
            print(f"ì±„íŒ… RAG ì‹¤í–‰ ì‹¤íŒ¨: {e}", flush=True)
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", flush=True)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            fallback_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{user_query}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            return {
                "user_query": user_query,
                "response": fallback_response,
                "sources": [],
                "source_count": 0,
                "success": False,
                "error": str(e)
            }

# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService() 