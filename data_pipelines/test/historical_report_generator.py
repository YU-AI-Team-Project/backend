import openai
import os
import csv
import pandas as pd
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func, text, create_engine
from dotenv import load_dotenv
from datetime import datetime, date

load_dotenv()

class HistoricalReportGenerator:
    def __init__(self):
        self.openai_client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            timeout=60.0
        )
        self.embedding_model = "text-embedding-3-small"
        self.chat_model = "gpt-4o"
        
        # MySQL ì—°ê²° ì„¤ì • (ì£¼ì‹/ì¬ë¬´ ë°ì´í„°)
        mysql_user = os.getenv("DB_USER")
        mysql_passwd = os.getenv("DB_PASSWORD")
        mysql_host = os.getenv("DB_HOST")
        mysql_port = os.getenv("DB_PORT")
        mysql_db = os.getenv("DB_NAME")
        
        if mysql_user and mysql_passwd and mysql_host and mysql_port and mysql_db:
            mysql_url = f"mysql+pymysql://{mysql_user}:{mysql_passwd}@{mysql_host}:{mysql_port}/{mysql_db}?charset=utf8mb4"
            self.mysql_engine = create_engine(mysql_url)
        else:
            print("âŒ MySQL ì—°ê²° ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            self.mysql_engine = None
        
        # PostgreSQL ì—°ê²° ì„¤ì • (ë‰´ìŠ¤ ë°ì´í„°)
        pg_user = os.getenv("NEWS_DB_USER")
        pg_passwd = os.getenv("NEWS_DB_PASSWORD")
        pg_host = os.getenv("NEWS_DB_HOST")
        pg_port = os.getenv("NEWS_DB_PORT")
        pg_db = os.getenv("NEWS_DB_NAME")
        
        if pg_user and pg_passwd and pg_host and pg_port and pg_db:
            pg_url = f'postgresql+psycopg2://{pg_user}:{pg_passwd}@{pg_host}:{pg_port}/{pg_db}'
            self.postgres_engine = create_engine(pg_url)
        else:
            # ëŒ€ì²´ ë°©ì‹: DATABASE_URL í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
            pg_url = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/dbname")
            self.postgres_engine = create_engine(pg_url)
        
        # 2024ë…„ ì´ì „ ë°ì´í„° ê¸°ì¤€ì¼ ì„¤ì •
        self.cutoff_date = date(2024, 1, 1)
        
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (rag_service.py ì°¸ê³ )"""
        if os.getenv("OPENAI_API_KEY") is None:
            print("âŒ OPENAI_API_KEY is None")
        try:
            print("ì„ë² ë”© ì‹œì‘")
            response = self.openai_client.embeddings.create(
                input=[text],
                model=self.embedding_model,
                timeout=30
            )
            print("ì„ë² ë”© ì™„ë£Œ")
            return response.data[0].embedding
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_diverse_queries(self, stock_code: str, stock_info=None) -> List[Dict[str, Any]]:
        """ì£¼ì‹ ë¶„ì„ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê³¼ê±° ë°ì´í„°ìš©)"""
        queries = []
        
        # 1. ê¸°ë³¸ ì¢…ëª© ì½”ë“œ ê²€ìƒ‰ (ë†’ì€ ì •í™•ë„)
        queries.append({
            "query": stock_code,
            "type": "stock_code",
            "top_k": 15,
            "threshold": 0.5
        })
        
        if stock_info:
            # 2. íšŒì‚¬ëª… ê²€ìƒ‰ (ì¤‘ê°„ ì •í™•ë„)
            if stock_info.get("company_name"):
                queries.append({
                    "query": stock_info["company_name"],
                    "type": "company_name", 
                    "top_k": 12,
                    "threshold": 0.4
                })
                
                # 3. íšŒì‚¬ëª… + ì£¼ìš” í‚¤ì›Œë“œ ì¡°í•©
                company_keywords = [
                    f"{stock_info['company_name']} ì‹¤ì ",
                    f"{stock_info['company_name']} ì „ë§",
                    f"{stock_info['company_name']} íˆ¬ì",
                    f"{stock_info['company_name']} ì£¼ê°€"
                ]
                for keyword in company_keywords:
                    queries.append({
                        "query": keyword,
                        "type": "company_keyword",
                        "top_k": 8,
                        "threshold": 0.3
                    })
            
            # 4. ì—…ì¢…/ì„¹í„° ê´€ë ¨ ê²€ìƒ‰
            if stock_info.get("industry"):
                queries.append({
                    "query": f"{stock_info['industry']} ì—…ì¢…",
                    "type": "industry",
                    "top_k": 8,
                    "threshold": 0.25
                })
                
            if stock_info.get("sector"):
                queries.append({
                    "query": f"{stock_info['sector']} ì„¹í„°",
                    "type": "sector", 
                    "top_k": 8,
                    "threshold": 0.25
                })
            
            # 5. ì‚¬ì—… ì˜ì—­ ê´€ë ¨ ê²€ìƒ‰
            if stock_info.get("business_summary"):
                business_keywords = self._extract_business_keywords(stock_info["business_summary"])
                for keyword in business_keywords[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    queries.append({
                        "query": keyword,
                        "type": "business_keyword",
                        "top_k": 6,
                        "threshold": 0.25
                    })
        
        # 6. ì¼ë°˜ì ì¸ ì£¼ì‹ ê´€ë ¨ í‚¤ì›Œë“œ
        general_keywords = [
            f"{stock_code} ë¶„ì„",
            f"{stock_code} ë¦¬í¬íŠ¸"
        ]
        for keyword in general_keywords:
            queries.append({
                "query": keyword,
                "type": "analysis_keyword",
                "top_k": 5,
                "threshold": 0.3
            })
        
        return queries
    
    def _extract_business_keywords(self, business_summary: str) -> List[str]:
        """ì‚¬ì—… ìš”ì•½ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not business_summary:
            return []
        
        # ì£¼ìš” ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œë“¤
        business_terms = [
            "ë°˜ë„ì²´", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í”Œë ˆì´", "ìŠ¤ë§ˆíŠ¸í°", "ì „ì", "IT", "ì†Œí”„íŠ¸ì›¨ì–´",
            "ë°”ì´ì˜¤", "ì œì•½", "í™”í•™", "ì„ìœ ", "ìë™ì°¨", "ì¡°ì„ ", "ê±´ì„¤", "ê¸ˆìœµ",
            "ì€í–‰", "ì¦ê¶Œ", "ë³´í—˜", "í†µì‹ ", "ê²Œì„", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ìœ í†µ",
            "ì‹í’ˆ", "ìŒë£Œ", "ì˜ë¥˜", "í™”ì¥í’ˆ", "í•­ê³µ", "ë¬¼ë¥˜", "ì—ë„ˆì§€",
            "AI", "ì¸ê³µì§€ëŠ¥", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "5G", "IoT", "ë¸”ë¡ì²´ì¸",
            "technology", "software", "hardware", "semiconductor", "biotech",
            "pharmaceutical", "automotive", "financial", "banking", "insurance"
        ]
        
        found_keywords = []
        business_lower = business_summary.lower()
        
        for term in business_terms:
            if term in business_summary or term.lower() in business_lower:
                found_keywords.append(term)
        
        return found_keywords[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€

    def get_historical_news(self, stock_code: str, db_session: Session, limit: int = 50) -> List[Dict[str, Any]]:
        """2024ë…„ ì´ì „ ë‰´ìŠ¤ ë°ì´í„° ê²€ìƒ‰ (ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì‚¬ìš©)"""
        try:
            # 1. ê¸°ì—… ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (MySQLì—ì„œ)
            stock_info = None
            with Session(self.mysql_engine) as mysql_session:
                stock_query = text("SELECT * FROM stocks WHERE code = :stock_code")
                stock_result = mysql_session.execute(stock_query, {"stock_code": stock_code}).fetchone()
                if stock_result:
                    stock_info = dict(stock_result._mapping)
            
            # 2. ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_queries = self._generate_diverse_queries(stock_code, stock_info)
            print(f"ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜: {len(search_queries)} for {stock_code}")
            
            # 3. ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ ìˆ˜ì§‘
            all_docs = []
            for i, query_info in enumerate(search_queries):
                print(f"ğŸ” ê²€ìƒ‰ {i+1}/{len(search_queries)}: {query_info['query']} (íƒ€ì…: {query_info['type']})")
                
                # 2024ë…„ ì´ì „ ë‰´ìŠ¤ ì²­í¬ ê²€ìƒ‰ with ë²¡í„° ìœ ì‚¬ë„
                query_text = text("""
                    SELECT id, title, chunk_text, published_at,
                           1 - (embedding <=> CAST(:query_embedding AS vector)) as similarity
                    FROM news_chunks
                    WHERE published_at < :cutoff_date
                    AND 1 - (embedding <=> CAST(:query_embedding AS vector)) > :threshold
                    ORDER BY embedding <=> CAST(:query_embedding AS vector)
                    LIMIT :limit
                """)
                
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = self.get_embedding(query_info["query"])
                if not query_embedding:
                    continue
                
                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
                
                result = db_session.execute(
                    query_text,
                    {
                        "query_embedding": embedding_str,
                        "cutoff_date": self.cutoff_date,
                        "threshold": query_info["threshold"],
                        "limit": query_info["top_k"]
                    }
                )
                
                docs = []
                for row in result:
                    docs.append({
                        "id": str(row.id),
                        "title": row.title,
                        "content": row.chunk_text,
                        "published_at": row.published_at.isoformat() if row.published_at else None,
                        "similarity": float(row.similarity),
                        "query_type": query_info["type"]
                    })
                
                print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ")
                if docs:
                    print(f"   ğŸ“Š ìœ ì‚¬ë„: {min([d['similarity'] for d in docs]):.3f} ~ {max([d['similarity'] for d in docs]):.3f}")
                
                all_docs.extend(docs)
            
            print(f"ì „ì²´ ê²€ìƒ‰ ì™„ë£Œ - ì´ ë¬¸ì„œ: {len(all_docs)}ê°œ")
            
            # 4. ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
            seen_ids = set()
            historical_news = []
            for doc in all_docs:
                if doc["id"] not in seen_ids:
                    seen_ids.add(doc["id"])
                    historical_news.append(doc)
            
            # 5. ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ limitê°œë§Œ ì„ íƒ
            historical_news = sorted(historical_news, key=lambda x: x["similarity"], reverse=True)[:limit]
            
            print(f"2024ë…„ ì´ì „ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(historical_news)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
            if historical_news:
                print(f"ìµœì¢… ìœ ì‚¬ë„ ë²”ìœ„: {min([d['similarity'] for d in historical_news]):.3f} ~ {max([d['similarity'] for d in historical_news]):.3f}")
                
                # ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
                type_counts = {}
                for doc in historical_news:
                    query_type = doc.get("query_type", "unknown")
                    type_counts[query_type] = type_counts.get(query_type, 0) + 1
                print(f"ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„í¬: {type_counts}")
            
            return historical_news
            
        except Exception as e:
            print(f"ê³¼ê±° ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
            return self._fallback_news_search(stock_code, db_session, limit)
    
    def _fallback_news_search(self, stock_code: str, db_session: Session, limit: int = 50) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            query_text = text("""
                SELECT id, title, chunk_text, published_at
                FROM news_chunks
                WHERE published_at < :cutoff_date
                AND (title ILIKE :stock_pattern OR chunk_text ILIKE :stock_pattern)
                ORDER BY published_at DESC
                LIMIT :limit
            """)
            
            result = db_session.execute(
                query_text,
                {
                    "cutoff_date": self.cutoff_date,
                    "stock_pattern": f"%{stock_code}%",
                    "limit": limit
                }
            )
            
            historical_news = []
            for row in result:
                historical_news.append({
                    "id": str(row.id),
                    "title": row.title,
                    "content": row.chunk_text,
                    "published_at": row.published_at.isoformat() if row.published_at else None,
                    "similarity": 0.5,  # ê¸°ë³¸ê°’
                    "query_type": "fallback"
                })
            
            print(f"ëŒ€ì²´ ê²€ìƒ‰ ê²°ê³¼: {len(historical_news)}ê°œ")
            return historical_news
            
        except Exception as e:
            print(f"ëŒ€ì²´ ê²€ìƒ‰ë„ ì‹¤íŒ¨: {e}")
            return []
    
    def get_historical_financial_data(self, stock_code: str, db_session: Session) -> Dict[str, Any]:
        """2024ë…„ ì´ì „ ì¬ë¬´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (MySQL)"""
        try:
            financial_data = {}
            
            # MySQL ì„¸ì…˜ ì‚¬ìš©
            with Session(self.mysql_engine) as mysql_session:
                # ì¢…ëª© ê¸°ë³¸ ì •ë³´
                stock_query = text("SELECT * FROM stocks WHERE code = :stock_code")
                stock_result = mysql_session.execute(stock_query, {"stock_code": stock_code}).fetchone()
                
                if stock_result:
                    financial_data["basic_info"] = dict(stock_result._mapping)
                
                # 2024ë…„ ì´ì „ ì¬ë¬´ì œí‘œ ë°ì´í„°
                financial_query = text("""
                    SELECT * FROM financial_statements 
                    WHERE stock_code = :stock_code 
                    AND report_period < :cutoff_date
                    ORDER BY report_period DESC
                    LIMIT 8
                """)
                
                financial_results = mysql_session.execute(
                    financial_query, 
                    {"stock_code": stock_code, "cutoff_date": "2024-01-01"}
                ).fetchall()
                
                financial_data["statements"] = [dict(row._mapping) for row in financial_results]
            
            print(f"ê³¼ê±° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {stock_code}")
            return financial_data
            
        except Exception as e:
            print(f"ê³¼ê±° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def generate_historical_report(self, stock_code: str) -> Dict[str, Any]:
        """2024ë…„ ì´ì „ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì£¼ì‹ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            with Session(self.postgres_engine) as db_session:
                # 1. ê³¼ê±° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
                historical_news = self.get_historical_news(stock_code, db_session)
                
                # 2. ê³¼ê±° ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘
                financial_data = self.get_historical_financial_data(stock_code, db_session)
                
                # 3. ê³¼ê±° ë°ì´í„° ë¶„ì„ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                analysis_prompt = """ë‹¹ì‹ ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ ì¶œì‹ ì˜ ì „ë¬¸ ê¸°ì—… ë¶„ì„ê°€ì…ë‹ˆë‹¤.

2024ë…„ ì´ì „ì˜ ê³¼ê±° ë‰´ìŠ¤ ë°ì´í„°ì™€ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì— ëŒ€í•œ ê³¼ê±° ì‹œì  ê¸°ì¤€ ì¢…í•©ì ì¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“Œ ì£¼ì–´ì§€ëŠ” ë°ì´í„°:
- 2024ë…„ ì´ì „ ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤
- 2024ë…„ ì´ì „ ìƒì„¸í•œ ì¬ë¬´ ì •ë³´ (ì¬ë¬´ì œí‘œ, ì‹œì¥ì§€í‘œ)

ğŸ¯ ë¶„ì„ ë³´ê³ ì„œ êµ¬ì„±:

## ğŸ“Š ê¸°ì—… ê°œìš”
- ì‚¬ì—… ë¶„ì•¼ ë° ì£¼ìš” ì‚¬ì—…
- ì—…ê³„ ë‚´ ìœ„ì¹˜

## ğŸ“ˆ ì¬ë¬´ ìƒí™© ë¶„ì„ (2024ë…„ ì´ì „ ê¸°ì¤€)
- ìˆ˜ìµì„± ì§€í‘œ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ë§ˆì§„ë¥  ë“±)
- ìˆ˜ìµì„± ì§€í‘œ ì„¤ëª…: ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì¹˜ì˜ ì˜ë¯¸, ê¸°ì¤€ì„ , ì—…ê³„ í‰ê·  ë¹„êµ ë“± ìì„¸í•œ í•´ì„¤ í¬í•¨
- ì¬ë¬´ê±´ì „ì„± (ë¶€ì±„ë¹„ìœ¨, ìœ ë™ë¹„ìœ¨, ROE/ROA ë“±)
- ì¬ë¬´ê±´ì „ì„± ì„¤ëª…: ê° ì§€í‘œì˜ ê°œë…, ìˆ˜ì¹˜ í•´ì„, ë¦¬ìŠ¤í¬ ì—¬ë¶€ ë“± ì´ˆë³´ì ì‹œê°ì—ì„œ ìì„¸í•œ ì„¤ëª… ì¶”ê°€
- ì„±ì¥ì„± (ë§¤ì¶œ/ì´ìµ ì¦ê°€ìœ¨)
- ì„±ì¥ì„± ì„¤ëª…: ì„±ì¥ì„± ìˆ˜ì¹˜ í•´ì„, ì‹œì¥ ë‚´ ì„±ì¥ ìœ„ì¹˜, í–¥í›„ ì „ë§ ë“± ìƒì„¸í•˜ê²Œ ê¸°ìˆ 
- ë°¸ë¥˜ì—ì´ì…˜ (PER, PBR, EV/EBITDA ë“±)
- ë°¸ë£¨ì—ì´ì…˜ ì„¤ëª…: í•´ë‹¹ ì§€í‘œì˜ ì˜ë¯¸, ê³ í‰ê°€/ì €í‰ê°€ ì—¬ë¶€, íˆ¬ì ë§¤ë ¥ë„ ê´€ì  ì„¤ëª…

## ğŸ“° ê³¼ê±° ì£¼ìš” ë™í–¥ ë° ì´ìŠˆ (2024ë…„ ì´ì „)
- ì£¼ìš” ë‰´ìŠ¤ ì´ë²¤íŠ¸ ìš”ì•½
- ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ ë¶„ì„
- ì‹œì¥ ë°˜ì‘ ë° ì˜í–¥ë„
- ê³¼ê±° ë™í–¥ ë° ì´ìŠˆ ì„¤ëª…: ê¸°ì—…ì— ì˜í–¥ì„ ì¤€ ê³¼ê±° ë‰´ìŠ¤, ì •ì±…, ê¸°ìˆ  ë™í–¥ì„ ì´ˆë³´ì ê´€ì ì—ì„œ ìƒì„¸íˆ í•´ì„

## ğŸ”® ê³¼ê±° ì‹œì  ê¸°ì¤€ ì „ë§ ë° ë¦¬ìŠ¤í¬
- ê³¼ê±° ë°ì´í„° ê¸°ì¤€ ì‹¤ì  íŒ¨í„´
- ì„±ì¥ ë™ë ¥ ë° ê¸°íšŒìš”ì¸
- ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸
- ì „ë§ ë° ë¦¬ìŠ¤í¬ ì„¤ëª…: ê³¼ê±° ë°ì´í„° ê¸°ì¤€ ê¸ì •/ë¶€ì • íŒ¨í„´ ê·¼ê±°ë¥¼ ì œì‹œí•˜ê³ , ì£¼ìš” ë¦¬ìŠ¤í¬ëŠ” ì´ˆë³´ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•´ì„¤

## ğŸ’¡ ì¢…í•© ì˜ê²¬ (ê³¼ê±° ë°ì´í„° ê¸°ì¤€)
- íˆ¬ì ë§¤ë ¥ë„: â­â­â­â­â­ (5ì  ë§Œì )
- íˆ¬ì í¬ì¸íŠ¸ ìš”ì•½
- ì£¼ì˜ ì‚¬í•­
- ê³¼ê±° ë°ì´í„° í•œê³„ì  ëª…ì‹œ

âš ï¸ ì‘ì„± ì›ì¹™:
- ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê° ìœ ì§€
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±° ì œì‹œ
- ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì„ì„ ëª…ì‹œ
- ì •ë³´ ì œê³µ ì¤‘ì‹¬ (êµ¬ì²´ì  ë§¤ë§¤ ì¡°ì–¸ ì§€ì–‘)
- ëª¨ë“  ë¶„ì„ì€ 2024ë…„ ì´ì „ ë°ì´í„°ì—ë§Œ ê¸°ë°˜"""
                
                # 4. ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                news_content = ""
                if historical_news:
                    news_summaries = []
                    for doc in historical_news:
                        news_summaries.append(f"â€¢ {doc['title']}: {doc['content'][:200]}...")
                    news_content = "\n".join(news_summaries)
                
                # 5. ì¬ë¬´ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                financial_content = ""
                if financial_data:
                    if financial_data.get("basic_info"):
                        basic_info = financial_data["basic_info"]
                        financial_content += f"â—† ê¸°ì—… ê¸°ë³¸ ì •ë³´:\n"
                        for key, value in basic_info.items():
                            if value is not None:
                                financial_content += f"  - {key}: {value}\n"
                    
                    if financial_data.get("statements"):
                        financial_content += f"\nâ—† ê³¼ê±° ì¬ë¬´ì œí‘œ (2024ë…„ ì´ì „):\n"
                        for stmt in financial_data["statements"]:
                            financial_content += f"  - ë³´ê³ ê¸°ê°„: {stmt.get('report_period', 'N/A')}\n"
                            for key, value in stmt.items():
                                if key not in ['id', 'stock_code', 'report_period'] and value is not None:
                                    financial_content += f"    {key}: {value}\n"
                            financial_content += "\n"
                
                # 6. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                user_content = f"""
[ë¶„ì„ ëŒ€ìƒ ì¢…ëª©]: {stock_code}
[ë¶„ì„ ê¸°ê°„]: 2024ë…„ ì´ì „ ë°ì´í„°

[ê³¼ê±° ë‰´ìŠ¤ ê¸°ì‚¬ë“¤]:
{news_content if news_content else "ê´€ë ¨ ê³¼ê±° ë‰´ìŠ¤ ì—†ìŒ"}

[ê³¼ê±° ì¬ë¬´ ì •ë³´]:
{financial_content if financial_content else "ê´€ë ¨ ê³¼ê±° ì¬ë¬´ ë°ì´í„° ì—†ìŒ"}

[ë¶„ì„ ìš”ì²­]: {stock_code} ê¸°ì—…ì— ëŒ€í•œ 2024ë…„ ì´ì „ ë°ì´í„° ê¸°ë°˜ ê³¼ê±° ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
                
                # 7. GPT ë¶„ì„ ì‹¤í–‰
                messages = [
                    {"role": "system", "content": analysis_prompt},
                    {"role": "user", "content": user_content}
                ]
                
                print(f"GPT ê³¼ê±° ë¶„ì„ ì‹œì‘: {stock_code}")
                response = self.openai_client.chat.completions.create(
                    model=self.chat_model,
                    messages=messages,
                    temperature=0,  # ìµœëŒ€ ì¼ê´€ì„±
                    max_tokens=12000
                )
                print(f"GPT ê³¼ê±° ë¶„ì„ ì™„ë£Œ: {stock_code}")
                
                report_content = response.choices[0].message.content
                
                return {
                    "stock_code": stock_code,
                    "analysis_type": "historical_analysis",
                    "data_period": "pre_2024",
                    "report": report_content,
                    "news_count": len(historical_news),
                    "financial_periods": len(financial_data.get("statements", [])),
                    "generated_at": datetime.now().isoformat(),
                    "success": True
                }
                
        except Exception as e:
            print(f"ê³¼ê±° ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            return {
                "stock_code": stock_code,
                "analysis_type": "historical_analysis",
                "data_period": "pre_2024",
                "report": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "news_count": 0,
                "financial_periods": 0,
                "generated_at": datetime.now().isoformat(),
                "success": False,
                "error": str(e)
            }
    
    def save_report_to_csv(self, report_data: Dict[str, Any], output_dir: str = "data"):
        """ë³´ê³ ì„œë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ê²½ë¡œ ìƒì„±
            script_dir = os.path.dirname(os.path.abspath(__file__))
            if not os.path.isabs(output_dir):
                output_dir = os.path.join(script_dir, output_dir)
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"historical_report_{report_data['stock_code']}_{timestamp}.csv"
            filepath = os.path.join(output_dir, filename)
            
            # CSVë¡œ ì €ì¥
            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'stock_code', 'analysis_type', 'data_period', 'report',
                    'news_count', 'financial_periods', 'generated_at', 'success'
                ]
                
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                # ì—ëŸ¬ ì •ë³´ ì œì™¸í•˜ê³  ì €ì¥
                save_data = {k: v for k, v in report_data.items() if k != 'error'}
                writer.writerow(save_data)
            
            print(f"ë³´ê³ ì„œ CSV ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def batch_generate_reports(self, stock_codes: List[str], output_dir: str = "data"):
        """ì—¬ëŸ¬ ì¢…ëª©ì— ëŒ€í•œ ì¼ê´„ ë³´ê³ ì„œ ìƒì„±"""
        results = []
        
        for i, stock_code in enumerate(stock_codes, 1):
            print(f"ë³´ê³ ì„œ ìƒì„± ì§„í–‰: {i}/{len(stock_codes)} - {stock_code}")
            
            # ë³´ê³ ì„œ ìƒì„±
            report_data = self.generate_historical_report(stock_code)
            
            # CSV ì €ì¥
            csv_path = self.save_report_to_csv(report_data, output_dir)
            report_data["csv_path"] = csv_path
            
            results.append(report_data)
            
            # ì§„í–‰ìƒí™© ë¡œê·¸
            if report_data["success"]:
                print(f"âœ… {stock_code} ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            else:
                print(f"âŒ {stock_code} ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨")
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½ CSV ìƒì„±
        self.save_batch_summary(results, output_dir)
        
        return results
    
    def save_batch_summary(self, results: List[Dict[str, Any]], output_dir: str):
        """ì¼ê´„ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ CSV ì €ì¥"""
        try:
            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ê²½ë¡œ ìƒì„±
            script_dir = os.path.dirname(os.path.abspath(__file__))
            if not os.path.isabs(output_dir):
                output_dir = os.path.join(script_dir, output_dir)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_filename = f"historical_reports_summary_{timestamp}.csv"
            summary_filepath = os.path.join(output_dir, summary_filename)
            
            # ìš”ì•½ ë°ì´í„° ì¤€ë¹„
            summary_data = []
            for result in results:
                summary_data.append({
                    "stock_code": result["stock_code"],
                    "success": result["success"],
                    "news_count": result["news_count"],
                    "financial_periods": result["financial_periods"],
                    "generated_at": result["generated_at"],
                    "csv_path": result.get("csv_path", ""),
                    "error": result.get("error", "")
                })
            
            # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            df = pd.DataFrame(summary_data)
            df.to_csv(summary_filepath, index=False, encoding='utf-8')
            
            print(f"ì¼ê´„ ì²˜ë¦¬ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_filepath}")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            total_count = len(results)
            success_count = sum(1 for r in results if r["success"])
            total_news = sum(r["news_count"] for r in results)
            total_financial_periods = sum(r["financial_periods"] for r in results)
            
            print(f"ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ í†µê³„:")
            print(f"  - ì „ì²´ ì¢…ëª©: {total_count}ê°œ")
            print(f"  - ì„±ê³µ: {success_count}ê°œ")
            print(f"  - ì‹¤íŒ¨: {total_count - success_count}ê°œ")
            print(f"  - ì´ ë‰´ìŠ¤ ìˆ˜: {total_news}ê°œ")
            print(f"  - ì´ ì¬ë¬´ ê¸°ê°„ ìˆ˜: {total_financial_periods}ê°œ")
            
        except Exception as e:
            print(f"ìš”ì•½ CSV ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = HistoricalReportGenerator()
    
    # configì—ì„œ ëœë¤ SP500 ì¢…ëª© 50ê°œ ê°€ì ¸ì˜¤ê¸°
    from config_historical import get_random_sp500_tickers
    
    # SP500ì—ì„œ ëœë¤ 250ê°œ ì¢…ëª© ì„ íƒ
    test_stock_codes = get_random_sp500_tickers(250)
    
    print("ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
    print(f"ëŒ€ìƒ ì¢…ëª©: ëœë¤ SP500 250ê°œ")
    print(f"ì„ íƒëœ ì¢…ëª© ì²˜ìŒ 10ê°œ: {test_stock_codes[:10]}")
    print(f"ë¶„ì„ ê¸°ì¤€: 2024ë…„ ì´ì „ ë°ì´í„°")
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜ì— data í´ë” ìƒì„±
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "data", "historical_reports")
    
    print(f"ì¶œë ¥ ìœ„ì¹˜: {output_dir}")
    
    # ì¼ê´„ ë³´ê³ ì„œ ìƒì„±
    results = generator.batch_generate_reports(
        stock_codes=test_stock_codes,
        output_dir=output_dir
    )
    
    print("ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")

if __name__ == "__main__":
    main() 