import os
import sys
import uuid
import time
import json
from datetime import datetime
from openai import OpenAI
from sqlalchemy import text, desc
from dotenv import load_dotenv
import tiktoken

# ê²½ë¡œ ì„¤ì •ì„ í†µí•´ aibackend íŒ¨í‚¤ì§€ë¥¼ import í•  ìˆ˜ ìˆë„ë¡ í•¨
backend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
sys.path.append(backend_path)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼)
dotenv_path = os.path.join(backend_path, '.env')
load_dotenv(dotenv_path)

# aibackendì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from aibackend.app.news_vector_db import NewsBase, NewsSessionLocal, save_news_vectors
from pgvector.sqlalchemy import Vector
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy import Column, Text, DateTime, Integer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHUNK_SIZE = 500  # ì²­í¬ í¬ê¸° (í† í° ë‹¨ìœ„)
CHUNK_OVERLAP = 200  # ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (í† í° ë‹¨ìœ„)
BATCH_SIZE = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë‰´ìŠ¤ ê°œìˆ˜
EMBEDDING_MODEL = "text-embedding-3-small"  # OpenAI ì„ë² ë”© ëª¨ë¸
ENCODING_NAME = "cl100k_base"  # OpenAI ì„ë² ë”© ëª¨ë¸ìš© ì¸ì½”ë”©

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print(f"OPENAI_API_KEY: {OPENAI_API_KEY}")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
encoding = tiktoken.get_encoding(ENCODING_NAME)

# ê¸°ì¡´ ë‰´ìŠ¤ ì„ë² ë”© ëª¨ë¸ (ì½ê¸°ìš©)
class NewsEmbedding(NewsBase):
    __tablename__ = 'news_vectors'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    embedding = Column(Vector(1536))
    published_at = Column(DateTime)
    
    def __repr__(self):
        return f"<NewsEmbedding(id='{self.id}', title='{self.title[:30]}...')>"

# ì²­í‚¹ëœ ë‰´ìŠ¤ ì„ë² ë”© ëª¨ë¸ (ì €ì¥ìš©)
class NewsChunkEmbedding(NewsBase):
    __tablename__ = 'news_chunks'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    news_id = Column(UUID(as_uuid=True), nullable=False)  # ì›ë³¸ ë‰´ìŠ¤ ID
    title = Column(Text, nullable=False)
    chunk_text = Column(Text, nullable=False)
    chunk_index = Column(Integer, nullable=False)  # ì²­í¬ ìˆœì„œ
    embedding = Column(Vector(1536))
    published_at = Column(DateTime)
    
    def __repr__(self):
        return f"<NewsChunkEmbedding(id='{self.id}', chunk_index={self.chunk_index}, title='{self.title[:30]}...')>"

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™” ë° pgvector í™•ì¥ í™•ì¸"""
    session = NewsSessionLocal()
    
    try:
        # pgvector í™•ì¥ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        result = session.execute(text("SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector')"))
        extension_exists = result.scalar()
        
        if not extension_exists:
            session.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            print("pgvector í™•ì¥ ìƒì„± ì™„ë£Œ")
            session.commit()
            
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        session.rollback()
    finally:
        session.close()

def get_embedding(text):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
    try:
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def chunk_text(text, max_tokens=CHUNK_SIZE, overlap_tokens=CHUNK_OVERLAP):
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì²­í‚¹"""
    # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
    tokens = encoding.encode(text)
    
    if len(tokens) <= max_tokens:
        # í…ìŠ¤íŠ¸ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(tokens):
        # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
        end = min(start + max_tokens, len(tokens))
        
        # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        chunk_tokens = tokens[start:end]
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)
        
        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (ê²¹ì¹˜ëŠ” ë¶€ë¶„ ê³ ë ¤)
        if end == len(tokens):
            break
        start = end - overlap_tokens
    
    return chunks

def fetch_existing_news_batch(session, offset=0, batch_size=BATCH_SIZE):
    """ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê°€ì ¸ì˜´"""
    query = session.query(NewsEmbedding).order_by(desc(NewsEmbedding.published_at))
    return query.offset(offset).limit(batch_size).all()

def get_total_news_count(session):
    """ì „ì²´ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ê°€ì ¸ì˜´"""
    return session.query(NewsEmbedding).count()

def save_chunk_embeddings(session, chunk_embeddings):
    """ì²­í‚¹ëœ ì„ë² ë”©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        session.add_all(chunk_embeddings)
        session.commit()
        print(f"âœ… {len(chunk_embeddings)}ê°œì˜ ì²­í¬ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"ì²­í¬ ì„ë² ë”© ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        session.rollback()
        raise

def process_news_chunking(limit=None):
    """ê¸°ì¡´ ë‰´ìŠ¤ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²­í‚¹í•˜ì—¬ ì„ë² ë”© ìƒì„± ë° ì €ì¥"""
    session = NewsSessionLocal()
    
    try:
        # ì „ì²´ ë‰´ìŠ¤ ê°œìˆ˜ í™•ì¸
        total_news_count = get_total_news_count(session)
        print(f"ì´ {total_news_count:,}ê°œì˜ ë‰´ìŠ¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ìˆìŠµë‹ˆë‹¤.")
        
        if limit:
            total_news_count = min(total_news_count, limit)
            print(f"ì²˜ë¦¬ ì œí•œìœ¼ë¡œ ì¸í•´ {total_news_count:,}ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ ID í™•ì¸
        processed_news_ids = set()
        existing_chunks = session.query(NewsChunkEmbedding.news_id).distinct().all()
        for (news_id,) in existing_chunks:
            processed_news_ids.add(news_id)
        
        print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(processed_news_ids):,}ê°œ")
        
        chunk_embeddings_to_save = []
        processed_count = 0
        batch_count = 0
        offset = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        while offset < total_news_count:
            batch_count += 1
            current_batch_size = min(BATCH_SIZE, total_news_count - offset)
            
            print(f"\nğŸ“¦ ë°°ì¹˜ {batch_count} ì²˜ë¦¬ ì¤‘ (ë‰´ìŠ¤ {offset+1}~{offset+current_batch_size})...")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‰´ìŠ¤ ë¡œë“œ
            news_batch = fetch_existing_news_batch(session, offset, current_batch_size)
            
            if not news_batch:
                print("ë” ì´ìƒ ì²˜ë¦¬í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            batch_processed = 0
            for news in news_batch:
                # ì´ë¯¸ ì²˜ë¦¬ëœ ë‰´ìŠ¤ëŠ” ê±´ë„ˆë›°ê¸°
                if news.id in processed_news_ids:
                    continue
                
                print(f"ğŸ”„ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘: {news.title[:50]}...")
                
                # ì œëª©ê³¼ ë‚´ìš©ì„ í•©ì³ì„œ ì²­í‚¹
                full_text = f"{news.title}\n\n{news.content}"
                chunks = chunk_text(full_text)
                
                print(f"ğŸ“„ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")
                
                # ê° ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
                for chunk_index, chunk_content in enumerate(chunks):
                    print(f"  ğŸ”„ ì²­í¬ {chunk_index + 1}/{len(chunks)} ì„ë² ë”© ìƒì„± ì¤‘...")
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = get_embedding(chunk_content)
                    if not embedding:
                        print(f"  âŒ ì²­í¬ {chunk_index + 1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆë›°ê¸°")
                        continue
                    
                    # ì²­í¬ ì„ë² ë”© ì—”í‹°í‹° ìƒì„±
                    chunk_embedding = NewsChunkEmbedding(
                        id=uuid.uuid4(),
                        news_id=news.id,
                        title=news.title,
                        chunk_text=chunk_content,
                        chunk_index=chunk_index,
                        embedding=embedding,
                        published_at=news.published_at
                    )
                    
                    chunk_embeddings_to_save.append(chunk_embedding)
                    
                    # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
                    time.sleep(0.1)
                
                batch_processed += 1
                processed_count += 1
                
                # 20ê°œ ì²­í¬ë§ˆë‹¤ DBì— ì €ì¥
                if len(chunk_embeddings_to_save) >= 20:
                    save_chunk_embeddings(session, chunk_embeddings_to_save)
                    chunk_embeddings_to_save = []
            
            print(f"âœ… ë°°ì¹˜ {batch_count} ì™„ë£Œ: {batch_processed}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ (ëˆ„ì : {processed_count}ê°œ)")
            offset += current_batch_size
        
        # ë‚¨ì€ ì²­í¬ ì„ë² ë”© ì €ì¥
        if chunk_embeddings_to_save:
            save_chunk_embeddings(session, chunk_embeddings_to_save)
        
        print(f"\nâœ… ì „ì²´ ë‰´ìŠ¤ ì²­í‚¹ ë° ì„ë² ë”© ì™„ë£Œ: {processed_count:,}ê°œ ì²˜ë¦¬")
        
    except Exception as e:
        print(f"ë‰´ìŠ¤ ì²­í‚¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        session.rollback()
    finally:
        session.close()

def get_chunk_stats():
    """ì²­í‚¹ í†µê³„ ì •ë³´ ì¶œë ¥"""
    session = NewsSessionLocal()
    
    try:
        # ì›ë³¸ ë‰´ìŠ¤ ìˆ˜
        original_count = session.query(NewsEmbedding).count()
        
        # ì²­í¬ ìˆ˜
        chunk_count = session.query(NewsChunkEmbedding).count()
        
        # ì²­í‚¹ëœ ë‰´ìŠ¤ ìˆ˜
        chunked_news_count = session.query(NewsChunkEmbedding.news_id).distinct().count()
        
        print(f"\nğŸ“Š ì²­í‚¹ í†µê³„:")
        print(f"  - ì›ë³¸ ë‰´ìŠ¤ ìˆ˜: {original_count:,}ê°œ")
        print(f"  - ì²­í‚¹ëœ ë‰´ìŠ¤ ìˆ˜: {chunked_news_count:,}ê°œ")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {chunk_count:,}ê°œ")
        print(f"  - í‰ê·  ì²­í¬ ìˆ˜ (ì²­í‚¹ëœ ë‰´ìŠ¤ë‹¹): {chunk_count / chunked_news_count:.1f}ê°œ" if chunked_news_count > 0 else "  - í‰ê·  ì²­í¬ ìˆ˜: N/A")
        
    except Exception as e:
        print(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        session.close()

def main():
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ì •í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("ë‰´ìŠ¤ ì²­í‚¹ ë° ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print(f"ì²­í¬ í¬ê¸°: {CHUNK_SIZE} í† í°")
        print(f"ì²­í¬ ê²¹ì¹¨: {CHUNK_OVERLAP} í† í°")
        print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        init_db()
        
        # í˜„ì¬ í†µê³„ ì¶œë ¥
        get_chunk_stats()
        
        # ë‰´ìŠ¤ ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬
        # limit ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ ì²˜ë¦¬í•  ë‰´ìŠ¤ ìˆ˜ë¥¼ ì œí•œí•  ìˆ˜ ìˆìŒ (í…ŒìŠ¤íŠ¸ ì‹œ ìœ ìš©)
        process_news_chunking(limit=None)  # Noneì´ë©´ ëª¨ë“  ë‰´ìŠ¤ ì²˜ë¦¬
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        get_chunk_stats()
        
        print("ë‰´ìŠ¤ ì²­í‚¹ ë° ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
