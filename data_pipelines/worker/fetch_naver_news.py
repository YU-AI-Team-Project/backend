import time, requests, pandas as pd
from bs4 import BeautifulSoup, Tag
from urllib.parse import urlparse, parse_qs, unquote
import random

response = requests.get("https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90")

html = response.text
soup = BeautifulSoup(html, "lxml")

articles = []

headers = {
    "User-Agent": "Mozilla/5.0"
}

def real_url(a: Tag) -> str:
    # 1) cruê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
    if a.has_attr("cru"):
        return a["cru"]

    # 2) href ì•ˆì˜ u= íŒŒë¼ë¯¸í„° ë””ì½”ë“œ
    qs = parse_qs(urlparse(a["href"]).query)
    if "u" in qs:
        return unquote(qs["u"][0])

    # 3) fallback â€“ ë§ˆì§€ë§‰ ë³´í˜¸ë§‰
    return a["href"]

for wrapper in soup.select('span.sds-comps-profile-info-subtext'):   # list<Tag>
    a_tag = wrapper.select_one('a[href]')        # Tag | None
    if not a_tag:                    # â€˜10ì‹œê°„ ì „â€™ ê°™ì´ ë§í¬ ì—†ëŠ” ê²½ìš°
        continue
    articles.append(
        {
            "press": a_tag.get_text(strip=True),  # ë„¤ì´ë²„ë‰´ìŠ¤
            "url": real_url(a_tag)
        }
    )

full_articles = []

for index, article in enumerate(articles):
    url = article["url"]

    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âš ï¸ {index+1}ë²ˆ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
        continue

    html = res.text
    soup = BeautifulSoup(html, "html.parser")

    try:
        title = soup.select_one("#title_area").get_text(strip=True)
        content = soup.select_one("#dic_area").get_text(strip=True)
    except AttributeError:
        print(f"âŒ ë³¸ë¬¸/ì œëª© ì„ íƒ ì‹¤íŒ¨: {url}")
        continue

    print(f"\n================ {index+1} ë²ˆì§¸ ê¸°ì‚¬ ================\n")
    print("ì œëª©:", title)
    print("ê¸°ì‚¬ ë‚´ìš©:\n", content[:200] + "..." if len(content) > 200 else content)

    full_articles.append({
        "title": title,
        "content": content,
        "press": article["press"],
        "url": url
    })

    time.sleep(random.uniform(1.5, 3.5))  # ì„œë²„ì— ë¶€ë‹´ ì£¼ì§€ ì•Šë„ë¡

# ğŸ’¾ STEP 3: ì €ì¥
df = pd.DataFrame(full_articles)
df.to_csv("naver_news.csv", index=False, encoding="utf-8-sig")
print("\nâœ… CSV ì €ì¥ ì™„ë£Œ: naver_news.csv")











