import time, requests, pandas as pd
from bs4 import BeautifulSoup, Tag
from urllib.parse import urlparse, parse_qs, unquote
import random
from urllib.parse import urljoin
import requests
import json, pathlib
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
START_DATE = "2025-01-30"
END_DATE = "2025-05-17"
BASE = "https://n.news.naver.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

#------------------------------í—¬í¼ í•¨ìˆ˜------------------------------
def daterange(start_date: str, end_date: str):
    d0 = datetime.fromisoformat(start_date)
    d1 = datetime.fromisoformat(end_date)
    delta = timedelta(days=1)
    while d0 <= d1:
        yield d0.strftime("%Y-%m-%d")
        d0 += delta

def news_read_to_mnews(url: str) -> str:
    """
    '/news/news_read.naver?article_id=0001209610&office_id=215&â€¦'
        â†’ 'https://n.news.naver.com/mnews/article/215/0001209610'
    """
    if "/news/news_read" not in url:
        return url                          # ëŒ€ìƒì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ

    qs  = parse_qs(urlparse(url).query)     # ì¿¼ë¦¬ â†’ dict
    oid = qs.get("office_id", [""])[0]
    aid = qs.get("article_id", [""])[0]
    return f"https://n.news.naver.com/mnews/article/{oid}/{aid}" if oid and aid else url
#------------------------------í—¬í¼ í•¨ìˆ˜------------------------------

#------------------------------ë©”ì¸ ë¡œì§------------------------------
all_rows = []

for date in daterange(START_DATE, END_DATE):
    time.sleep(random.uniform(1.5, 3.0))
    file_name = f"naverfinance_mainnews_{date}.csv"
    print(f"ğŸ“„ {file_name} ìˆ˜ì§‘ ì‹œì‘")

    first_url = f"https://finance.naver.com/news/mainnews.naver?date={date}"
    first_resp = requests.get(first_url, headers=HEADERS, timeout=10)
    soup_first = BeautifulSoup(first_resp.text, "lxml")

    a = soup_first.select_one("td.pgRR a[href]")
    qs = parse_qs(urlparse(a["href"]).query)
    max_page = int(qs.get("page", [1])[0])

    print(f"ìµœëŒ€ í˜ì´ì§€: {max_page}")

    articles = []

    for page_no in range(1, max_page + 1):
        time.sleep(random.uniform(1.5, 3.0))
        url = f"https://finance.naver.com/news/mainnews.naver?date={date}&page={page_no}"
        response = requests.get(url, headers=HEADERS, timeout = 10)
        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url}")
            continue
        soup = BeautifulSoup(response.text, "lxml")
        print(f"\nğŸ”„ [ëª©ë¡] page {page_no} ìš”ì²­ â†’ {url}")

        for li in soup.select("div.mainNewsList li.block1"):   # ì¹´ë“œ ì „ë¶€ ìˆœíšŒ
            title_a = li.select_one("dd.articleSubject > a[href]")
            press   = li.select_one("dd.articleSummary span.press")
            wdate   = li.select_one("dd.articleSummary span.wdate")
            if not title_a:
                continue

            url = news_read_to_mnews(title_a["href"])
            articles.append({
                "title": title_a.get_text(strip=True),
                "url":   url,         # ì´ë¯¸ https://n.news.naver.com/â€¦
                "press": press.get_text(strip=True) if press else "",
                "time":  wdate.get_text(strip=True) if wdate else ""
            })
            
        print(f"ğŸ“„ page {page_no} ìˆ˜ì§‘ ì™„ë£Œ (ê¸°ì‚¬ {len(articles)}ê°œ)")

    rows = []
    for idx, art in enumerate(articles, start=1):
        time.sleep(random.uniform(1.5, 3.0))        # ë”œë ˆì´

        res = requests.get(art["url"], headers=HEADERS, timeout=10)
        if res.status_code != 200:
            print("âŒ ìš”ì²­ ì‹¤íŒ¨", art["url"])
            continue

        page = BeautifulSoup(res.text, "html.parser")
        body = page.select_one("#dic_area, .go_trans._article_content")
        art["content"] = body.get_text(strip=True) if body else ""
        rows.append(art)

        print(f"âœ… page {idx} ë³¸ë¬¸ ì €ì¥")
    
        # ğŸ’¾ STEP 3: ì €ì¥
    df = pd.DataFrame(rows)
    df.to_csv(file_name, index=False, encoding="utf-8-sig")
    print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {file_name}")

        # ğŸ’¾ STEP 4: JSONL
    path = pathlib.Path(file_name.replace(".csv", ".jsonl"))
    with path.open("w", encoding="utf-8") as f:
        for art in rows:
            f.write(json.dumps(art, ensure_ascii=False) + "\n")
    print(f"\nâœ… JSONL ì €ì¥ ì™„ë£Œ: {path}")

    all_rows.extend(rows)