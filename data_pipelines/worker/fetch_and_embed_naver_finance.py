import os
import sys
import uuid
import time
import random
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta
from openai import OpenAI
from sqlalchemy import text
from dotenv import load_dotenv

# ê²½ë¡œ ì„¤ì •ì„ í†µí•´ aibackend íŒ¨í‚¤ì§€ë¥¼ import í•  ìˆ˜ ìˆë„ë¡ í•¨
backend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
sys.path.append(backend_path)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼)
dotenv_path = os.path.join(backend_path, '.env')
load_dotenv(dotenv_path)

# aibackendì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from aibackend.app.news_vector_db import NewsBase, NewsSessionLocal, save_news_vectors
from pgvector.sqlalchemy import Vector
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy import Column, Text, DateTime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
START_DATE = "2025-02-21"
END_DATE = "2025-05-17"
BASE = "https://n.news.naver.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}
EMBEDDING_MODEL = "text-embedding-3-small"  # OpenAI ì„ë² ë”© ëª¨ë¸

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print(f"OPENAI_API_KEY: {OPENAI_API_KEY}")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# ë‰´ìŠ¤ ì„ë² ë”©ì„ ìœ„í•œ ëª¨ë¸ ì •ì˜
class NewsEmbedding(NewsBase):
    __tablename__ = 'news_vectors'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(Text, nullable=False)
    content = Column(Text, nullable=False)
    embedding = Column(Vector(1536))
    published_at = Column(DateTime)
    
    def __repr__(self):
        return f"<NewsEmbedding(id='{self.id}', title='{self.title[:30]}...')>"

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™” ë° pgvector í™•ì¥ í™•ì¸"""
    # ì„¸ì…˜ ìƒì„±
    session = NewsSessionLocal()
    
    try:
        # pgvector í™•ì¥ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        result = session.execute(text("SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector')"))
        extension_exists = result.scalar()
        
        if not extension_exists:
            session.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            print("pgvector í™•ì¥ ìƒì„± ì™„ë£Œ")
            session.commit()
        
        print("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        session.rollback()
    finally:
        session.close()

def get_embedding(text):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
    try:
        # API ì œí•œì„ ìœ„í•´ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        if len(text) > 100000:
            text = text[:100000]
            
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def daterange(start_date: str, end_date: str):
    d0 = datetime.fromisoformat(start_date)
    d1 = datetime.fromisoformat(end_date)
    delta = timedelta(days=1)
    while d0 <= d1:
        yield d0.strftime("%Y-%m-%d")
        d0 += delta

def news_read_to_mnews(url: str) -> str:
    """
    '/news/news_read.naver?article_id=0001209610&office_id=215&â€¦'
        â†’ 'https://n.news.naver.com/mnews/article/215/0001209610'
    """
    if "/news/news_read" not in url:
        return url                          # ëŒ€ìƒì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ

    qs  = parse_qs(urlparse(url).query)     # ì¿¼ë¦¬ â†’ dict
    oid = qs.get("office_id", [""])[0]
    aid = qs.get("article_id", [""])[0]
    return f"https://n.news.naver.com/mnews/article/{oid}/{aid}" if oid and aid else url

def fetch_and_embed_news():
    """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ì„ë² ë”©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„±
    session = NewsSessionLocal()
    
    # ë‚ ì§œë³„ë¡œ í¬ë¡¤ë§
    for date in daterange(START_DATE, END_DATE):
        time.sleep(random.uniform(1.5, 3.0))
        print(f"ğŸ“… {date} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

        first_url = f"https://finance.naver.com/news/mainnews.naver?date={date}"
        first_resp = requests.get(first_url, headers=HEADERS, timeout=10)
        soup_first = BeautifulSoup(first_resp.text, "lxml")

        # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ í™•ì¸
        a = soup_first.select_one("td.pgRR a[href]")
        if not a:
            print("í˜ì´ì§€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        qs = parse_qs(urlparse(a["href"]).query)
        max_page = int(qs.get("page", [1])[0])
        print(f"ìµœëŒ€ í˜ì´ì§€: {max_page}")

        articles = []
        news_embeddings_to_save = []
        count = 0

        # í˜ì´ì§€ë³„ë¡œ ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘
        for page_no in range(1, max_page + 1):
            time.sleep(random.uniform(1.5, 3.0))
            url = f"https://finance.naver.com/news/mainnews.naver?date={date}&page={page_no}"
            response = requests.get(url, headers=HEADERS, timeout=10)
            if response.status_code != 200:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url}")
                continue
                
            soup = BeautifulSoup(response.text, "lxml")
            print(f"\nğŸ”„ [ëª©ë¡] page {page_no} ìš”ì²­ â†’ {url}")

            # ë‰´ìŠ¤ í•­ëª© ì¶”ì¶œ
            for li in soup.select("div.mainNewsList li.block1"):
                time.sleep(random.uniform(1.5, 3.0))
                title_a = li.select_one("dd.articleSubject > a[href]")
                press = li.select_one("dd.articleSummary span.press")
                wdate = li.select_one("dd.articleSummary span.wdate")
                if not title_a:
                    continue

                url = news_read_to_mnews(title_a["href"])
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
                try:
                    time.sleep(random.uniform(1.5, 3.0))
                    print(f"ğŸ”„ [ë³¸ë¬¸] ìš”ì²­ â†’ {url}")
                    res = requests.get(url, headers=HEADERS, timeout=10)
                    if res.status_code != 200:
                        print(f"âŒ ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {url}")
                        continue
                        
                    page = BeautifulSoup(res.text, "html.parser")
                    body = page.select_one("#dic_area, .go_trans._article_content")
                    
                    title = title_a.get_text(strip=True)
                    content = body.get_text(strip=True) if body else ""
                    
                    # ì„ë² ë”© ìƒì„±
                    print(f"ğŸ”„ [ì„ë² ë”©] ìƒì„± ì¤‘...")
                    embedding_text = f"{title}\n{content}"
                    if not embedding_text.strip():
                        print(f"ë‰´ìŠ¤ '{title}'ì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœë‹ˆë‹¤")
                        continue
                        
                    embedding = get_embedding(embedding_text)
                    if not embedding:
                        print(f"ë‰´ìŠ¤ '{title}'ì˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ê±´ë„ˆëœë‹ˆë‹¤")
                        continue
                    
                    # ë°œí–‰ì¼ íŒŒì‹±
                    published_at = None
                    time_str = wdate.get_text(strip=True) if wdate else ""
                    if time_str:
                        try:
                            if isinstance(time_str, str):
                                published_at = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                            elif isinstance(time_str, (int, float)):
                                published_at = datetime.fromtimestamp(time_str)
                        except:
                            print(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {time_str}")
                            published_at = datetime.now()
                    else:
                        published_at = datetime.now()
                    
                    # ë‰´ìŠ¤ ì—”í‹°í‹° ìƒì„±
                    news_id = uuid.uuid4()
                    news_embedding = NewsEmbedding(
                        id=news_id,
                        title=title,
                        content=content,
                        published_at=published_at,
                        embedding=embedding
                    )
                    
                    news_embeddings_to_save.append(news_embedding)
                    count += 1
                    print(f"âœ… {len(news_embeddings_to_save)}ê°œì˜ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ (ì´ {count}ê°œ)")
                    
                    # 10ê°œë§ˆë‹¤ DBì— ì €ì¥
                    if len(news_embeddings_to_save) >= 10:
                        try:
                            save_news_vectors(session, news_embeddings_to_save)
                            print(f"âœ… {len(news_embeddings_to_save)}ê°œì˜ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ (ì´ {count}ê°œ)")
                            news_embeddings_to_save = []
                        except Exception as e:
                            print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            session.rollback()
                
                except Exception as e:
                    print(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            print(f"ğŸ“„ page {page_no} ìˆ˜ì§‘ ë° ì„ë² ë”© ì™„ë£Œ")
        
        # ë‚¨ì€ ë‰´ìŠ¤ ì €ì¥
        if news_embeddings_to_save:
            try:
                save_news_vectors(session, news_embeddings_to_save)
                print(f"âœ… ë‚¨ì€ {len(news_embeddings_to_save)}ê°œì˜ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ (ì´ {count}ê°œ)")
            except Exception as e:
                print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                session.rollback()
        
        print(f"ğŸ“… {date} ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì„ë² ë”© ì™„ë£Œ (ì´ {count}ê°œ)")
    
    # ì„¸ì…˜ ì¢…ë£Œ
    session.close()

def main():
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ì •í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        init_db()
        
        # ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì„ë² ë”© ì €ì¥
        fetch_and_embed_news()
        
        print("ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
    except Exception as e:
        print(f"ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main() 